```py
# The usual imports

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
```

```python
# load the data
  #定义一个图像预处理流程，将图像转换为张量并重塑为一维向量。 a
class ReshapeTransform:
    def __init__(self, new_size):
        self.new_size = new_size #初始化方法，接受目标形状 new_size 作为参数。

    def __call__(self, img):
        return torch.reshape(img, self.new_size) #使用 torch.reshape 将图像张量重塑为 new_size 指定的形状。

transformations = transforms.Compose([
                                transforms.ToTensor(), #将图像转换为 PyTorch 张量，形状为 [C, H, W]，
                                transforms.ConvertImageDtype(torch.float32),
    #将张量的数据类型转换为 torch.float32。
                                ReshapeTransform((-1,))  #使用自定义的 ReshapeTransform 类，将图像张量展平为一维向量。
                                ])

trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transformations)

testset = torchvision.datasets.MNIST(root='./data', train=False,
                                       download=True, transform=transformations)#使用自定义的transformations将MNIST图像转换成张亮并展平成一维向量
```

```py
trainset.data.shape, testset.data.shape
#训练集包含 60000 张图像，每张图像的形状为 (28, 28)。测试集包含 10000 张图像，每张图像的形状为 (28, 28)。

# data loader

BATCH_SIZE = 128 #表示每次加载 128 个样本。
train_dataloader = torch.utils.data.DataLoader(trainset, #训练数据集
                                               batch_size=BATCH_SIZE,
                                               shuffle=True, #开始时打乱数据顺序
                                               num_workers=0)

test_dataloader = torch.utils.data.DataLoader(testset, 
                                              batch_size=BATCH_SIZE,
                                              shuffle=False, 
                                              num_workers=0)


# model
定义一个全连接神经网络模型。
model = nn.Sequential(nn.Linear(784, 512), nn.ReLU(), nn.Linear(512, 10))
 #nn.Sequential:将多个网络层按顺序组合在一起，形成一个完整的模型。
  #nn.Linear(784, 512):输入维度为 784，输出维度为 512。输入维度为 784，输出维度为 512。  nn.ReLU()激活函数，引入非线性。公式为：ReLU(x) = max(0, x)
nn.Linear(512, 10): 第二个全连接层，输入维度为 512，输出维度为 10
10 是 MNIST 数据集的类别数（0 到 9）



# training preparation

trainer = torch.optim.RMSprop(model.parameters()) #定义优化器，用于更新模型参数。
	#model.parameters():获取模型的所有可训练参数（权重和偏置）。
 	#RMSprop 是一种自适应学习率优化算法，适合处理非平稳目标函数。
loss = nn.CrossEntropyLoss() #定义损失函数，用于计算模型预测值与真实标签之间的差异。


def get_accuracy(output, target, batch_size):
   
    #output:模型的输出，形状为 [batch_size, 10]，表示每个样本的 10 个类别的原始得分（未经过 Softmax 激活）。
    #target:真实标签，形状为 [batch_size]，表示每个样本的真实类别索引（0 到 9）。
    #当前批次的样本数量
    corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()
    #torch.max(output, 1)[1] 对 output 在维度 1（类别维度）上取最大值，返回最大值对应的类别索引。  
    
    accuracy = 100.0 * corrects/batch_size
    return accuracy.item()
```



```py
# train

for ITER in range(5):
    train_acc = 0.0 #用于累积当前轮次的训练准确率
    train_running_loss = 0.0 #用于累积当前轮次的训练损失

    model.train() #将模型设置为训练模式。
    for i, (X, y) in enumerate(train_dataloader): #X：输入数据，形状为 [batch_size, 784]（MNIST 图像展平后的数据）  y：真实标签，形状为 [batch_size]（每个样本的类别索引）
        output = model(X)  #output：模型的输出，形状为 [batch_size, 10]，表示每个样本的 10 个类别的原始得分（未经过 Softmax 激活）。
        l = loss(output, y) #算模型预测值与真实标签之间的损失。

        # update the parameters #反向传播和参数更新：
        l.backward() #计算损失函数关于模型参数的梯度
        trainer.step()  #据梯度更新模型参数。
        trainer.zero_grad()

        # gather metrics
        train_acc += get_accuracy(output, y, BATCH_SIZE) #计算当前批次的准确率，并累加到 train_acc
        train_running_loss += l.detach().item()

    print('Epoch: %d | Train loss: %.4f | Train Accuracy: %.4f' \
          %(ITER+1, train_running_loss / (i+1),train_acc/(i+1)))
```

