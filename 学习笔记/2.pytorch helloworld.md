```python
## The usual imports
import torch
import torch.nn as nn

## print out the pytorch version used
print(torch.__version__)


## our data in tensor form
x = torch.tensor([[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]], dtype=torch.float)
y = torch.tensor([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]], dtype=torch.float)   
 x 和 y 是形状为 (6, 1) 的张量，数据类型为 float。数据关系为线性：y =  2x - 1（例如，当 x = -1 时，y = -3）。

## print size of the input tensor
x.size()

## Neural network with 1 hidden layer
layer1 = nn.Linear(1,1, bias=False)  nn.Linear(1, 1, bias=False)
   定义一个线性层，输入和输出维度均为1，且无偏置项（即 y = w*x）。
    
    
model = nn.Sequential(layer1)
nn.Sequential(layer1) 将层组合成顺序模型，此处模型仅包含单个线性层。


## loss function
criterion = nn.MSELoss()
使用均方误差（Mean Squared Error, MSE）作为损失函数，用于衡量模型输出与真实值的差异。


## optimizer algorithm
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
  使用随机梯度下降（SGD）优化器，学习率为 0.01，优化目标是模型的参数（model.parameters()）。

## training
for ITER in range(150):
    model = model.train()

    ## forward
    output = model(x)  前向传播 输入数据 x 通过模型得到预测值
    loss = criterion(output, y)   计算预测值与真实值的损失
    optimizer.zero_grad()  反向传播与参数更新 清空梯度，避免梯度累积

    ## backward + update model params 
    loss.backward()    反向传播计算梯度
    optimizer.step()  根据梯度更新模型参数

    model.eval()  打印损失
    print('Epoch: %d | Loss: %.4f' %(ITER, loss.detach().item()))
```

 a
