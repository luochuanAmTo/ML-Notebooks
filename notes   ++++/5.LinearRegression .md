### 线性回归：

```python
## Import the usual libraries
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing #从 scikit-learn 中加载加州房价数据集。
from sklearn.model_selection import train_test_split  #将数据集划分为训练集和测试集。
from sklearn.preprocessing import StandardScaler  #对数据进行标准化处理。
import matplotlib.pyplot as plt
```



```python
# Fetch the data using sklearn function
bunch = fetch_california_housing(download_if_missing=True, as_frame=True)
  #数据集加载为 Pandas DataFrame 格式
# Load the dataframe and view
df = bunch.frame
df.head()
```

![image-20250228111603374](C:\Users\AmTo2\AppData\Roaming\Typora\typora-user-images\image-20250228111603374.png)

df.discribe()  #**生成数据集的描述性统计信息 ** 包括数值型数据的分布、中心趋势、离散程度等。

- **`count`**：非空值的数量。
- **`mean`**：平均值。
- **`std`**：标准差，反映数据的离散程度。
- **`min`**：最小值。
- **`25%`**：第一四分位数（25% 分位数）。
- **`50%`**：中位数（50% 分位数）。
- **`75%`**：第三四分位数（75% 分位数）。
- **`max`**：最大值。

![image-20250228112316420](C:\Users\AmTo2\AppData\Roaming\Typora\typora-user-images\image-20250228112316420.png)

```python
# !wget https://raw.githubusercontent.com/Ankit152/Fish-Market/main/Fish.csv
# import pandas as pd
# df  = pd.read_csv("Fish.csv")
# y = df['Weight']
# x = df[["Length1", "Length2", "Length3", "Height", "Width","Weight"]]

df = bunch.frame  #从 Bunch 对象中提取数据集，并将其转换为 Pandas DataFrame。
x = df.iloc[:,:-1] # 选择所有列，除了最后一列
y = df.iloc[:,-1:] # 选择所有行和最后一列，作为目标值 y
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 1)   #将数据集划分为训练集和测试集

input_scalar = StandardScaler() #用于对数据进行标准化处理（均值为 0，标准差为 1）
output_scalar = StandardScaler()

x_train = input_scalar.fit_transform(x_train).T # 对训练集特征进行标准化处理a
x_test = input_scalar.transform(x_test).T # 将标准化后的数据进行转置（将行和列互换） values of train data  。通过标准化，可以确保测试集和训练集的数据分布一致

y_train = output_scalar.fit_transform(y_train).reshape(-1)#对训练集目标值进行标准化处理。将目标值转换为一维数组
y_test = output_scalar.transform(y_test).reshape(-1)

dataset_copy = [ x_train.copy(), x_test.copy(),  y_train.copy(),  y_test.copy()] #创建数据的副本  dataset_copy：存储标准化后的训练集和测试集的副本。
```

### **示例**

#### **示例数据**

假设训练集和测试集数据如下：

- **训练集特征**（`x_train`）：

  复制

  ```
  [[1, 2],
   [3, 4],
   [5, 6]]
  ```

- **测试集特征**（`x_test`）：

  复制

  ```
  [[7, 8],
   [9, 10]]
  ```

#### **标准化过程**

1. **拟合训练集**：

   - 计算训练集的均值和标准差：
     - 第 1 列：均值 μ1=3*μ*1=3，标准差 σ1=1.633*σ*1=1.633。
     - 第 2 列：均值 μ2=4*μ*2=4，标准差 σ2=1.633*σ*2=1.633。

2. **标准化测试集**：

   - 对测试集数据进行标准化：

     - 第 1 列：

       xscaled=7−31.633=2.449*x*scaled=1.6337−3=2.449

       xscaled=9−31.633=3.674*x*scaled=1.6339−3=3.674

     - 第 2 列：

       xscaled=8−41.633=2.449*x*scaled=1.6338−4=2.449

       xscaled=10−41.633=3.674*x*scaled=1.63310−4=3.674

   - 标准化后的测试集：

     ```
     [[2.449, 2.449],
      [3.674, 3.674]]
     ```

     

```python
class LinearRegression():  #线性回归模型
  def __init__(self, dim, lr = 0.1): #初始化类的实例 是特征的维度（即权重向量的长度） lr 是学习率（learning rate），默认值为 0.1
    assert isinstance
    self.lr = lr   
    self.w = np.zeros((dim))  #初始化权重向量 self.w，并将其设置为全零向量，长度为 dim
    self.grads = {"dw": np.zeros((dim)) +5}  #初始化梯度字典 self.grads

  def forward(self, x): # 定义前向传播方法 forward，用于计算模型的预测值 ,x 是输入数据，通常是一个特征向量或矩阵
    y = self.w.T @ x    #计算预测值 y，通过权重向量 self.w 的转置与输入 x 的矩阵乘法得到。
    return y #返回预测值 y
  
  def backward(self, x, y_hat, y): #定义反向传播方法 backward，用于计算梯度
    y_hat 是模型的预测值。y 是真实标签。
    assert y_hat.shape == y.shape  #断言 y_hat 和 y 的形状相同，确保预测值和真实标签的维度一致。
    self.grads["dw"] = (1 / x.shape[1]) * ((y_hat - y) @ x.T).T #计算权重 w 的梯度 dw
    assert self.grads["dw"].shape == self.w.shape #断言梯度 dw 的形状与权重 w 的形状相同，确保梯度计算正确。
    
    # print(self.grads["dw"])

  def optimize(self):
    self.w = self.w - self.lr * self.grads["dw"]
  #使用梯度下降法更新权重 w
```



在线性回归中，损失函数通常采用均方误差（Mean Squared Error, MSE）：
$$
L = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^{2}
$$
为了最小化损失函数，我们需要计算损失函数对权重 w的梯度
$$
\frac{\partial L}{\partial w} = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right) x^{(i)}
$$
梯度的方向是损失函数增长最快的方向，因此负梯度方向是损失函数下降最快的方向。通过沿着负梯度方向调整 w，可以逐步减小损失函数的值
$$
w = w - \eta \cdot \frac{\partial L}{\partial w}
$$

```python
num_epochs = 10000  #设置训练的迭代次数（epochs）为 10000
train_loss_history = []
test_loss_history = []  #初始化两个空列表，用于存储训练损失和测试损失的历史值。
w_history = []  #用于存储权重w的历史值
dim = x_train.shape[0]   #获取训练数据 x_train 的特征维度（即权重w的长度）
num_train = x_train.shape[1]  #获取训练数据 x_train 的样本数量
num_test = x_test.shape[1]  #获取测试数据 x_test 的样本数量


model = LinearRegression(dim = dim, lr = 0.1)  #创建一个 LinearRegression 模型的实例，设置特征维度为 dim，学习率为 0.1
for i in range(num_epochs):  #开始训练循环，迭代 num_epochs 次
  y_hat = model.forward(x_train) #使用当前权重 w对训练数据 x_train 进行前向传播，计算预测值 y_hat
  train_loss = 1/(2 * num_train) * ((y_train - y_hat) ** 2).sum()#计算训练损失（均方误差）

  w_history.append(model.w)  #将当前权重w添加到 w_history 列表中，用于记录权重的变化。
  model.backward(x_train,y_hat,y_train) #调用反向传播方法，计算损失函数对权重 w的梯度。
  model.optimize()  #调用优化方法，根据梯度更新权重w

  y_hat = model.forward(x_test) #使用更新后的权重 w对测试数据 x_test 进行前向传播，计算预测值 y_hat
  test_loss = 1/(2 * num_test) * ((y_test - y_hat) ** 2).sum()
  #  使用更新后的权重 w对测试数据 x_test 进行前向传播，计算预测值 y_hat。
  train_loss_history.append(train_loss) #将当前训练损失添加到 train_loss_history 列表中。
  test_loss_history.append(test_loss) #将当前测试损失添加到 test_loss_history 列表中。

  if i % 20 == 0: #每 20 次迭代打印一次当前训练损失和测试损失。
    print(f"Epoch {i} | Train Loss {train_loss} | Test Loss {test_loss}")
    
plt.plot(range(num_epochs), train_loss_history, label = "Training")
plt.plot(range(num_epochs), test_loss_history, label = "Test")
plt.legend()
plt.show()
```

![image-20250228184018072](C:\Users\AmTo2\AppData\Roaming\Typora\typora-user-images\image-20250228184018072.png)





```python
from sklearn.metrics import mean_squared_error #用于计算均方误差
y_test = output_scalar.inverse_transform(y_test[np.newaxis,:])
y_hat  = output_scalar.inverse_transform(y_hat[np.newaxis,:])
error = (((y_test - y_hat) ** 2).sum() / num_test )
print("Test Set Error", error)
#将标准化后的真实值 y_test 和预测值 y_hat 反标准化，恢复为原始尺度。
#计算测试集上的均方误差（MSE），衡量模型在测试集上的预测性能。
#输出测试集的误差值。


```

![image-20250228185738131](C:\Users\AmTo2\AppData\Roaming\Typora\typora-user-images\image-20250228185738131.png)

### 使用库

与从头开始编写所有内容（即模型、损失函数和梯度计算）相比，有许多库已经为我们实现了许多机器学习算法。

这些库通常会更快并且经过更优化。我们可以使用 scikit-learn 中的 LinearRegression 和 SGD Regressor 模块来比较我们的模型。

```python
from sklearn.linear_model import SGDRegressor
#SGDRegressor，这是一个基于随机梯度下降算法的线性回归模型。

x_train, x_test, y_train, y_test = dataset_copy #dataset_copy 中加载训练集和测试集：
sgd = SGDRegressor()#创建一个 SGDRegressor 的实例 sgd SGDRegressor 使用均方误差（MSE）作为损失函数，并通过随机梯度下降法优化模型参数。
sgd.fit(x_train.T, y_train)  #使用训练集数据训练模型： 将训练集特征数据转置，使其形状符合 SGDRegressor 的输入要求（样本数量在第一个维度，特征数量在第二个维度
y_hat = sgd.predict(x_test.T) #用训练好的模型对测试集进行预测
y_test = output_scalar.inverse_transform(y_test[np.newaxis,:]) #结果 y_test 是反标准化后的真实值
y_hat  = output_scalar.inverse_transform(y_hat[np.newaxis,:]) #结果 y_hat 是反标准化后的预测值
error = mean_squared_error(y_test, y_hat, squared = True) #返回均方误差（MSE）
print("Test Set Error", error)
```



```python
from sklearn.linear_model import LinearRegression as LR 
 #LinearRegression 是一个基于普通最小二乘法（OLS）的线性回归模型
x_train, x_test, y_train, y_test = dataset_copy
lr = LR()  #创建一个 LinearRegression 的实例 lr，用于训练线性回归模型。
lr.fit(x_train.T, y_train)
y_hat = lr.predict(x_test.T) #训练模型
y_test = output_scalar.inverse_transform(y_test[np.newaxis,:])
y_hat  = output_scalar.inverse_transform(y_hat[np.newaxis,:])
error = mean_squared_error(y_test, y_hat, squared = True)
print("Test Set Error", error)
```

