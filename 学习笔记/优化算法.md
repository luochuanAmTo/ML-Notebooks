## 动量梯度下降(Moment SGD)

- **普通SGD**：每次只看当前点的坡度决定方向

- **带动量的SGD**：记住之前的运动方向，形成"惯性"，使更新更平滑

```
v = γ * v - α * ∇J(θ)  # 更新速度(γ是动量系数，通常0.9)
θ = θ + v              # 更新参数
```

`v`：速度向量(累积历史梯度)

`γ`：动量系数(0-1之间，控制惯性大小)

`α`：学习率

`∇J(θ)`：当前梯度

## Adagrad自适应学习率算法

```py
optimizer = tf.keras.optimizers.Adagrad(
    learning_rate=0.1,
    initial_accumulator_value=0.1, 
    epsilon=1e-07 #ε：极小值（通常1e-8）防止除零
) 
```

y方向梯度大，学习率自动减小更快；x方向梯度小，学习率下降慢

## Adam优化算法

Adam（Adaptive Moment Estimation）就像"结合了动量和自适应学习率的智能优化器"，它同时考虑了：

**一阶矩估计（动量）**：梯度方向的指数加权平均（惯性，）**二阶矩估计（自适应学习率）**：梯度平方的指数加权平均（调整步长）

```py
import torch
import torch.optim as optim

# 定义推荐模型
class RecSysModel(torch.nn.Module):
    def __init__(self, num_users, num_items):
        super().__init__()
        self.user_embed = torch.nn.Embedding(num_users, 64)
        self.item_embed = torch.nn.Embedding(num_items, 64)
        self.fc = torch.nn.Linear(128, 1)# 创建全连接层：输入维度128(用户和物品嵌入拼接后的维度)，输出维度1(预测得分)
    
    def forward(self, user, item):
        u = self.user_embed(user)
        i = self.item_embed(item)
        return torch.sigmoid(self.fc(torch.cat([u, i], dim=1))) #将用户和物品嵌入向量在维度1上拼接(得到128维向量) 通过全连接层 应用sigmoid函数将输出转换为0-1之间的概率值

    
    
    
    
# 初始化
model = RecSysModel(num_users=1000, num_items=10000)   
optimizer = optim.Adam(model.parameters(),     #model.parameters()获取所有可训练参数
                      lr=0.001,    #lr=0.001设置学习率
                      betas=(0.9, 0.999))   #betas=(0.9, 0.999)是Adam优化器的超参数

# 数据样例
train_loader = torch.utils.data.DataLoader(  #创建数据加载器，用于批量加载训练数据
    dataset=[(torch.tensor(1), torch.tensor(101), torch.tensor(1)),
             (torch.tensor(2), torch.tensor(102), torch.tensor(0))],
    batch_size=32
)

# 训练循环
for user, item, label in train_loader: #遍历数据加载器，每次获取一个批量的(user, item, label) 
    optimizer.zero_grad()
    output = model(user, item) #前向传播：将用户和物品输入模型，得到预测输出 
    loss = torch.nn.functional.binary_cross_entropy(output, label.float()) #计算二元交叉熵损失
    loss.backward() #反向传播：计算梯度
    optimizer.step() #优化器更新模型参数
```

激活函数：将神经网络上一层的输入，经过神经网络层的非线性变换转换后，通过激活函数，得到输出。常见的激活函数包括：sigmoid, tanh, relu等。

损失函数：度量神经网络的输出的预测值，与实际值之间的差距的一种方式。常见的损失函数包括：最小二乘损失函数、交叉熵损失函数、回归中使用的smooth L1损失函数等。

优化函数：也就是如何把损失值从神经网络的最外层传递到最前面。如最基础的梯度下降算法，随机梯度下降算法，批量梯度下降算法，带动量的梯度下降算法，Adagrad，Adadelta，Adam等。

